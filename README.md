# [Visual_Question_Answering]()

Visual Question Answering (VQA) is an emerging field of artificial intelligence that aims to develop systems capable of answering natural language questions about images or videos. VQA is gaining significant attention due to its potential to enable machines to better understand and interact with the visual world. It has various applications, such as image and video search, robotics, and visual accessibility for people with disabilities.

Data-driven approaches are commonly used for building VQA systems and rely on deep learning techniques such as convolutional neural networks (CNNs) for image feature extraction and recurrent neural networks (RNNs) for language modeling. Rule-based approaches are also used, but data-driven approaches are more prevalent. In this project, we have used two approaches:

> 1.  VGG16 (Image) + Glove Vectors (Text Embeddings)
> 2. ResNet 50 (Image) + Bert (Text Embeddings)

Several VQA systems have been developed by researchers and companies, including VQA models proposed by VQA@VizWiz, Visual Dialog, and Dynamic Co-Attention Networks.

The VQA community is active, and several conferences and workshops are dedicated to VQA research, such as the annual Visual Question Answering and Dialog (VISDIAL) workshop and the Conference on Computer Vision and Pattern Recognition (CVPR). 

We have used the [Visualqa dataset](https://visualqa.org/download.html) for images and Text annotations.

In conclusion, VQA is an active research area, and new approaches and models are continuously being developed. It has great potential in various applications and is expected to play an essential role in the development of more advanced AI systems.

References:

1. Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., & Parikh, D. (2017). Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 6904-6913.
2. Ren, Y., Kiros, R., & Zemel, R. S. (2015). Exploring models and data for image question answering. Advances in Neural Information Processing Systems, 2953-2961.
3. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Lawrence Zitnick, C., & Parikh, D. (2015). VQA: Visual Question Answering. Proceedings of the IEEE International Conference on Computer Vision, 2425-2433.
4. Gurari, D., Li, Q., Stangl, A. J., Guo, H., & Tamara L. Berg (2018). VizWiz Grand Challenge: Answering Visual Questions from Blind People. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 3608-3617.
5. Das, A., Kottur, S., Gupta, K., Singh, A., Yadav, D., Moura, J. M. F., ... & Batra, D. (2017). Visual Dialog. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1080-1089.
6. Lu, J., Yang, J., Batra, D., & Parikh, D. (2016). Hierarchical question-image co-attention for visual question answering. Advances in Neural Information Processing Systems, 289-297.
7. Visual Question Answering and Dialog (VISDIAL) workshop
8. Conference on Computer Vision and Pattern Recognition (CVPR)

